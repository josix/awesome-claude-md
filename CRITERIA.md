# Curation Criteria for CLAUDE.md Examples

This document provides transparent, objective criteria for evaluating `claude.md` files for inclusion in the awesome-claude-md collection. Our goal is to maintain consistently high quality while ensuring fair and predictable evaluation.

## üìä Quick Reference

### Minimum Requirements (Must Meet All)
- ‚úÖ Repository has ‚â•100 stars OR from recognized organization
- ‚úÖ Active maintenance (commits within 6 months)
- ‚úÖ Open source with clear licensing
- ‚úÖ CLAUDE.md file demonstrates clear value for AI assistance

### Evaluation Scoring (0-100 points)
| Category | Weight | Max Points |
|----------|--------|------------|
| [Repository Quality](#repository-quality) | 25% | 25 |
| [Documentation Excellence](#documentation-excellence) | 30% | 30 |
| [AI-Friendly Structure](#ai-friendly-structure) | 25% | 25 |
| [Uniqueness & Learning Value](#uniqueness--learning-value) | 20% | 20 |

**Acceptance Threshold:** ‚â•70 points for inclusion

---

## üèóÔ∏è Repository Quality (25 points)

Evaluates the overall quality and credibility of the source repository.

### Scoring Breakdown

**Star Count & Recognition (10 points)**
- 10 pts: 10,000+ stars OR top-tier organization (Anthropic, Microsoft, Google, etc.)
- 8 pts: 5,000-9,999 stars OR well-known organization (Cloudflare, Sentry, etc.)
- 6 pts: 1,000-4,999 stars OR established organization
- 4 pts: 500-999 stars OR recognized individual contributor
- 2 pts: 100-499 stars
- 0 pts: <100 stars (auto-reject unless exceptional circumstances)

**Maintenance & Activity (8 points)**
- 8 pts: Daily/weekly commits, active issue resolution
- 6 pts: Monthly commits, responsive maintenance
- 4 pts: Commits within 3 months, basic maintenance
- 2 pts: Commits within 6 months
- 0 pts: No commits in 6+ months (auto-reject)

**Production Usage & Impact (4 points)**
- 4 pts: Major production usage, enterprise adoption
- 3 pts: Significant community adoption
- 2 pts: Active user base, production-ready
- 1 pt: Used in some production contexts
- 0 pts: Experimental/personal project only

**License & Legal (3 points)**
- 3 pts: MIT, Apache 2.0, or other permissive license
- 2 pts: GPL or other copyleft license (clearly documented)
- 1 pt: Custom license but allows public reference
- 0 pts: No license or restrictive terms (auto-reject)

---

## üìö Documentation Excellence (30 points)

Evaluates the quality and comprehensiveness of the CLAUDE.md content.

### Scoring Breakdown

**Clarity & Structure (12 points)**
- 12 pts: Exceptional organization, clear sections, logical flow
- 9 pts: Well-organized with clear structure
- 6 pts: Generally well-structured with minor gaps
- 3 pts: Basic structure but could be clearer
- 0 pts: Poor organization or confusing structure

**Completeness & Depth (10 points)**
- 10 pts: Comprehensive coverage of architecture, setup, workflows, and context
- 8 pts: Covers most essential areas with good detail
- 6 pts: Covers key areas but missing some important details
- 4 pts: Basic coverage of main topics
- 2 pts: Limited coverage, significant gaps
- 0 pts: Minimal or inadequate documentation

**Technical Accuracy (8 points)**
- 8 pts: Technically accurate throughout, up-to-date information
- 6 pts: Generally accurate with minor issues
- 4 pts: Mostly accurate but some outdated or unclear information
- 2 pts: Some technical inaccuracies
- 0 pts: Significant technical errors or outdated information

---

## ü§ñ AI-Friendly Structure (25 points)

Evaluates how well the documentation serves AI assistants.

### Scoring Breakdown

**Context Provision (10 points)**
- 10 pts: Rich context about project goals, architecture, and workflows
- 8 pts: Good context with clear project understanding
- 6 pts: Adequate context for basic understanding
- 4 pts: Limited context, some gaps
- 2 pts: Minimal context provided
- 0 pts: Insufficient context for AI assistance

**Command & Code Examples (8 points)**
- 8 pts: Extensive, accurate examples covering major workflows
- 6 pts: Good examples for most common tasks
- 4 pts: Basic examples covering some key areas
- 2 pts: Limited examples
- 0 pts: Few or no practical examples

**Architecture Description (7 points)**
- 7 pts: Detailed architecture with component relationships
- 5 pts: Clear architecture overview
- 3 pts: Basic architectural information
- 1 pt: Minimal architectural details
- 0 pts: No clear architecture description

---

## üéØ Uniqueness & Learning Value (20 points)

Evaluates what unique insights this example provides to the collection.

### Scoring Breakdown

**Novel Patterns or Techniques (8 points)**
- 8 pts: Demonstrates unique, advanced patterns not seen elsewhere
- 6 pts: Shows interesting techniques with some uniqueness
- 4 pts: Good practices but similar to existing examples
- 2 pts: Standard practices, limited novelty
- 0 pts: No unique value or patterns

**Educational Value (7 points)**
- 7 pts: High learning value for multiple developer levels
- 5 pts: Good learning value for most developers
- 3 pts: Some learning value for specific audiences
- 1 pt: Limited educational benefit
- 0 pts: Little to no educational value

**Fills Collection Gap (5 points)**
- 5 pts: Addresses underrepresented category or technology
- 3 pts: Adds to less-represented area
- 1 pt: Fits existing categories but adds value
- 0 pts: Duplicate of existing coverage

---

## üìã Evaluation Checklists

### For Contributors

Use this checklist when suggesting new examples:

**Pre-Submission Check:**
- [ ] Repository meets minimum star count (‚â•100) or is from recognized organization
- [ ] Project has been active within the last 6 months
- [ ] CLAUDE.md file exists and is substantial (not just a placeholder)
- [ ] Repository has clear, permissive licensing
- [ ] No similar examples already exist in the collection

**Quality Assessment:**
- [ ] CLAUDE.md provides clear project context and architecture
- [ ] Documentation includes practical commands and examples
- [ ] Content demonstrates best practices or unique techniques
- [ ] Structure would be helpful for AI assistants
- [ ] Example fills a gap in our current collection

### For Maintainers

Use this detailed rubric for objective evaluation:

**Repository Quality (25 points):**
- [ ] Star count: ___/10 points
- [ ] Maintenance: ___/8 points  
- [ ] Production usage: ___/4 points
- [ ] License: ___/3 points
- **Subtotal: ___/25 points**

**Documentation Excellence (30 points):**
- [ ] Clarity & structure: ___/12 points
- [ ] Completeness & depth: ___/10 points
- [ ] Technical accuracy: ___/8 points
- **Subtotal: ___/30 points**

**AI-Friendly Structure (25 points):**
- [ ] Context provision: ___/10 points
- [ ] Command & code examples: ___/8 points
- [ ] Architecture description: ___/7 points
- **Subtotal: ___/25 points**

**Uniqueness & Learning Value (20 points):**
- [ ] Novel patterns: ___/8 points
- [ ] Educational value: ___/7 points
- [ ] Fills collection gap: ___/5 points
- **Subtotal: ___/20 points**

**TOTAL SCORE: ___/100 points**

**Decision:** 
- [ ] Accept (‚â•70 points)
- [ ] Request improvements (60-69 points)
- [ ] Decline (<60 points)

---

## üé® Category-Specific Guidelines

### Complex Projects
- Focus on multi-service architecture and orchestration
- Emphasize service communication patterns
- Look for sophisticated deployment strategies

### Libraries & Frameworks  
- Highlight API design and usage patterns
- Emphasize testing strategies and best practices
- Look for extensibility and plugin architectures

### Developer Tooling
- Focus on CLI design and user experience
- Emphasize configuration and customization options
- Look for integration with existing developer workflows

### Project Handoffs
- Prioritize current state documentation and blocking issues
- Look for clear transition planning and knowledge transfer
- Emphasize maintainer guidance and contribution onboarding

### Getting Started
- Focus on onboarding experience and setup clarity
- Emphasize beginner-friendly documentation
- Look for progressive learning paths

### Infrastructure Projects
- Focus on scalability and operational concerns
- Emphasize deployment and monitoring patterns
- Look for production-readiness considerations

---

## üö´ Common Disqualifiers

**Automatic Rejection Criteria:**
- Repository has <100 stars AND is not from a recognized organization
- No commits in the last 6 months
- No license or restrictive licensing terms
- CLAUDE.md is just a placeholder or minimal content
- Significant technical inaccuracies or outdated information
- Duplicate of existing collection content

**Red Flags (Require Extra Scrutiny):**
- Single contributor with no community involvement
- No production usage evidence
- Documentation is mostly auto-generated
- Limited learning value for general audience
- Overly specific to single use case

---

## üîÑ Review Process

### 1. Initial Screening
- Verify minimum requirements
- Check for obvious disqualifiers
- Confirm CLAUDE.md exists and has substance

### 2. Detailed Evaluation  
- Score all four categories using the rubric
- Document specific strengths and weaknesses
- Consider fit with existing collection

### 3. Decision & Feedback
- **‚â•70 points:** Accept with praise for specific strengths
- **60-69 points:** Request specific improvements before acceptance
- **<60 points:** Decline with constructive feedback

### 4. Community Input
- For borderline cases (65-75 points), consider community feedback
- Tag relevant experts for technical domain assessment
- Allow 1-week comment period for community input

---

## üìà Continuous Improvement

This criteria document is living and should evolve based on:

- **Community Feedback:** Regular review of application and effectiveness
- **Collection Growth:** Adjust criteria as collection matures and gaps are filled
- **Industry Changes:** Update standards as development practices evolve
- **User Needs:** Adapt based on how the collection is being used

**Review Schedule:** Quarterly review of criteria effectiveness and potential updates.

---

## üìû Questions or Appeals

- **Criteria Questions:** Open an issue with the "question" label
- **Evaluation Appeals:** Comment on the relevant issue with specific concerns
- **Criteria Improvements:** Submit suggestions via issues or discussions

Our goal is fair, transparent, and consistent evaluation that serves the developer community's learning needs.