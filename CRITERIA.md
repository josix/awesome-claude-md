# Curation Criteria for CLAUDE.md Examples

This document defines the objective criteria for evaluating and selecting CLAUDE.md examples for inclusion in the awesome-claude-md collection.

## Overview

To ensure consistent quality and educational value, all submissions are evaluated using a structured scoring system across multiple dimensions. Examples must meet minimum thresholds in each category to be considered "awesome."

## Evaluation Rubric

### 1. Repository Quality & Recognition (25 points)

**Excellent (20-25 points):**
- 1,000+ GitHub stars OR from recognized industry organization
- Active maintenance (commits within last 3 months)
- Production usage evidence (documented users, case studies, etc.)
- Strong community engagement (active issues, PRs, discussions)

**Good (15-19 points):**
- 500+ GitHub stars OR from established open-source organization
- Regular maintenance (commits within last 6 months)
- Clear production usage indicators
- Moderate community activity

**Acceptable (10-14 points):**
- 100+ GitHub stars OR notable author/organization
- Maintained (commits within last year)
- Evidence of real-world usage

**Below Threshold (0-9 points):**
- Insufficient stars, maintenance, or usage evidence

### 2. CLAUDE.md Content Quality (30 points)

#### Architecture & Context (10 points)
- **Excellent (9-10):** Comprehensive system overview, clear component relationships, architectural diagrams
- **Good (7-8):** Good system context with component descriptions
- **Acceptable (5-6):** Basic architecture information provided
- **Poor (0-4):** Minimal or unclear architectural context

#### Development Workflow (10 points)
- **Excellent (9-10):** Complete command reference, environment setup, testing procedures, deployment guidance
- **Good (7-8):** Good coverage of essential development tasks
- **Acceptable (5-6):** Basic commands and setup instructions
- **Poor (0-4):** Incomplete or unclear workflow information

#### Technical Depth (10 points)
- **Excellent (9-10):** Detailed technical explanations, code examples, specific patterns demonstrated
- **Good (7-8):** Good technical detail with examples
- **Acceptable (5-6):** Adequate technical information
- **Poor (0-4):** Superficial or vague technical content

### 3. AI Assistant Effectiveness (25 points)

#### Information Structure (10 points)
- **Excellent (9-10):** Logical organization, clear sections, consistent formatting
- **Good (7-8):** Well-organized with minor issues
- **Acceptable (5-6):** Generally organized but could be clearer
- **Poor (0-4):** Disorganized or difficult to navigate

#### Actionable Instructions (10 points)
- **Excellent (9-10):** Specific, executable commands and procedures
- **Good (7-8):** Clear instructions with minor gaps
- **Acceptable (5-6):** Generally actionable with some ambiguity
- **Poor (0-4):** Vague or non-actionable content

#### Context Completeness (5 points)
- **Excellent (5):** Comprehensive context for AI understanding
- **Good (4):** Good context coverage
- **Acceptable (3):** Adequate context
- **Poor (0-2):** Insufficient context

### 4. Educational Value (20 points)

#### Unique Patterns & Techniques (10 points)
- **Excellent (9-10):** Demonstrates innovative or advanced patterns not commonly seen
- **Good (7-8):** Shows good practices with some unique elements
- **Acceptable (5-6):** Demonstrates standard good practices
- **Poor (0-4):** No distinctive techniques or patterns

#### Learning Potential (10 points)
- **Excellent (9-10):** High potential for developers to learn and apply concepts
- **Good (7-8):** Good learning value with practical applications
- **Acceptable (5-6):** Some educational benefit
- **Poor (0-4):** Limited learning potential

## Scoring Thresholds

- **Automatic Inclusion:** 85+ points (Exceptional quality)
- **Strong Candidate:** 70-84 points (Review for inclusion)
- **Conditional:** 55-69 points (Requires improvement or special consideration)
- **Not Suitable:** Below 55 points

## Category-Specific Considerations

### Complex Projects
- Must demonstrate multi-service architecture or enterprise-scale concerns
- Should show sophisticated deployment and scaling patterns
- Extra weight given to service orchestration and integration patterns

### Developer Tooling
- Must include comprehensive command documentation
- Should demonstrate plugin/extension patterns where applicable
- Build system and workflow automation highly valued

### Libraries & Frameworks
- Must include clear API documentation and usage examples
- Should demonstrate integration patterns and best practices
- Testing and quality assurance practices strongly valued

### Infrastructure Projects
- Must document deployment and operational concerns
- Should include monitoring, logging, and maintenance procedures
- Security and scalability patterns highly valued

### Getting Started
- Must provide clear onboarding experience
- Should include environment setup and common workflows
- Beginner-friendly explanations without sacrificing technical accuracy

### Project Handoffs
- Must document current state and blocking issues clearly
- Should include transition planning and next steps
- Knowledge transfer effectiveness is key criteria

## Evaluation Checklist

When evaluating a potential example, use this checklist:

### Pre-Evaluation Screening
- [ ] Repository has appropriate license (MIT, Apache 2.0, etc.)
- [ ] CLAUDE.md file is publicly accessible
- [ ] Repository shows signs of active maintenance
- [ ] No obvious copyright or licensing issues

### Quality Assessment
- [ ] Score Repository Quality & Recognition (0-25 points)
- [ ] Score CLAUDE.md Content Quality (0-30 points)
- [ ] Score AI Assistant Effectiveness (0-25 points)
- [ ] Score Educational Value (0-20 points)
- [ ] Calculate total score (0-100 points)

### Final Review
- [ ] Total score meets threshold requirements
- [ ] Example fits appropriately in intended category
- [ ] No similar examples already exist in collection
- [ ] Educational value justifies inclusion

### Documentation Requirements
- [ ] Analysis.md file would provide clear educational value
- [ ] Specific techniques can be identified and explained
- [ ] Key takeaways are actionable for developers

## Special Considerations

### Diversity and Representation
- Prioritize examples from diverse organizations and maintainers
- Balance representation across different technologies and approaches
- Consider global perspective and different development cultures

### Maintenance and Sustainability
- Prefer examples from projects with long-term viability
- Consider the ongoing maintenance burden of tracking changes
- Evaluate the stability of the documented patterns and practices

### Innovation Factor
- Give additional consideration to cutting-edge approaches
- Value examples that push the boundaries of AI-assisted development
- Recognize novel solutions to common problems

## Review Process

1. **Initial Screening:** Verify basic eligibility requirements
2. **Detailed Evaluation:** Complete scoring rubric assessment
3. **Peer Review:** Have assessment reviewed by another contributor
4. **Final Decision:** Document rationale for inclusion/exclusion
5. **Analysis Creation:** Develop comprehensive analysis.md explaining educational value

## Appeals Process

If a submission is rejected, contributors may:
1. Request detailed feedback on scoring
2. Improve the example based on feedback
3. Resubmit with explanations of improvements
4. Appeal the decision with additional context

## Continuous Improvement

These criteria will be periodically reviewed and updated based on:
- Community feedback and contributions
- Evolution of best practices in the field
- Changes in AI assistant capabilities
- Emerging patterns in effective CLAUDE.md files

---

*This criteria document is designed to be objective and transparent while maintaining the high quality standards that make this collection valuable to the community.*